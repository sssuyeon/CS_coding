{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from scipy.stats import randint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = []\n",
    "y_tr = []\n",
    "X_ts = []\n",
    "y_ts = []\n",
    "\n",
    "classnum = range(1032)\n",
    "for i in classnum :\n",
    "    file_train_names = '{}/{}/*.png'.format(\"syllable_train\", i)\n",
    "    for image in glob.glob(file_train_names) :\n",
    "        train_image = Image.open(image)\n",
    "        X_tr.append(np.array(train_image))\n",
    "        y_tr.append(i)\n",
    "        \n",
    "    file_inf_names = '{}/{}/*.png'.format(\"syllable_inference\", i)\n",
    "    for image in glob.glob(file_inf_names) :\n",
    "        ts_image = Image.open(image)\n",
    "        X_ts.append(np.array(ts_image))\n",
    "        y_ts.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 축을 따라 배열 쌓기 (동일한 shape)\n",
    "X_tr = np.stack(X_tr)\n",
    "X_ts = np.stack(X_ts)\n",
    "\n",
    "y_tr = np.array(y_tr)\n",
    "y_ts = np.array(y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization/flat operation (to make the 2D input into 1D vector)\n",
    "X_tr_flat = X_tr.reshape(X_tr.shape[0],-1)\n",
    "X_ts_flat = X_ts.reshape(X_ts.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_tr_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환\n",
    "X_pc_tr = pca.transform(X_tr_flat)\n",
    "X_pc_ts = pca.transform(X_ts_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc = sum(pca.explained_variance_ratio_.cumsum() < 0.7) #48\n",
    "\n",
    "X_pc_tr = X_pc_tr[:,:n_pc]\n",
    "X_pc_ts = X_pc_ts[:,:n_pc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41280, 3072)\n",
      "(41280, 48)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr_flat.shape)\n",
    "print(X_pc_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def hypertuning(model, params, iters, X, y):\n",
    "    rscv_search = RandomizedSearchCV(model, param_distributions = params, n_iter = iters, cv = 5, random_state = 4242)\n",
    "    rscv_search.fit(X, y)\n",
    "    DT_params = rscv_search.best_params_\n",
    "    DT_score = rscv_search.best_score_\n",
    "    return DT_params, DT_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "DT_untuned = tree.DecisionTreeClassifier(random_state = 4242)\n",
    "DT_untuned = DT_untuned.fit(X_pc_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_DT_untuned_tr = DT_untuned.score(X_pc_tr, y_tr)\n",
    "score_DT_untuned_val = cross_val_score(DT_untuned, X_pc_tr, y_tr, cv = 5).mean()\n",
    "score_DT_untuned_ts = DT_untuned.score(X_pc_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 기본 하이퍼 파라미터:\n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 4242, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# 기본 hyperparameter 추출\n",
    "\n",
    "print('DecisionTreeClassifier 기본 하이퍼 파라미터:\\n', DT_untuned.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_params = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : randint(40, 80), \n",
    "    'min_samples_split' : randint(2, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tuned = tree.DecisionTreeClassifier()\n",
    "DT_tuned_result = hypertuning(DT_tuned, DT_params, 10, X_tr_flat, y_tr)\n",
    "\n",
    "DT_tuned.set_params(**DT_tuned_result[0])\n",
    "\n",
    "score_DT_tuned_tr = DT_tuned.score(X_pc_tr, y_tr)\n",
    "score_DT_tuned_val = DT_tuned_result[1]\n",
    "score_DT_tuned_ts = DT_tuned.score(X_pc_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph = export_graphviz(DT_untuned,   \n",
    "                             out_file = \"tree.dot\", max_depth = 2,\n",
    "                           feature_names = ['PC'+str(i+1) for i in range(X_pc_tr.shape[1])],  \n",
    "                           class_names = ['Y'+str(i) for i in range(1032)],  \n",
    "                           filled = True,           \n",
    "                           rounded = True,          \n",
    "                           special_characters = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_untuned = LogisticRegression(random_state = 4242)\n",
    "LR_untuned = LR_untuned.fit(X_pc_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_LR_untuned_tr = LR_untuned.score(X_pc_tr, y_tr)\n",
    "score_LR_untuned_val = cross_val_score(LR_untuned, X_pc_tr, y_tr, cv = 5).mean()\n",
    "score_LR_untuned_ts = LR_untuned.score(X_pc_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 hyperparameter 추출\n",
    "\n",
    "print('LogisticRegression 기본 하이퍼 파라미터:\\n', LR_untuned.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_params = {\n",
    "    'penalty' : ['none', 'l1', 'l2', 'elasticnet']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tuned = LogisticRegression(random_state = 4242)\n",
    "\n",
    "LR_tuned_result = hypertuning(LR_tuned, LR_params, 10, X_pc_tr, y_tr)\n",
    "\n",
    "LR_tuned.set_params(**LR_tuned_result[0])\n",
    "\n",
    "score_LR_tuned_tr = LR_tuned.score(X_pc_tr, y_tr)\n",
    "score_LR_tuned_val = LR_tuned_result[1]\n",
    "score_LR_tuned_ts = LR_tuned.score(X_pc_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_untuned = SVC(random_state = 4242)\n",
    "SVM_untuned = SVM_untuned.fit(X_pc_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_SVM_untuned_tr = SVM_untuned.score(X_pc_tr, y_tr)\n",
    "score_SVM_untuned_val = cross_val_score(SVM_untuned, X_pc_tr, y_tr, cv = 5).mean()\n",
    "score_SVM_untuned_ts = SVM_untuned.score(X_pc_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_params = {\n",
    "    'C' : randint(3, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_tuned = SVC(random_state = 4242)\n",
    "\n",
    "SVM_tuned_result = hypertuning(SVM_tuned, SVM_params, 10, X_pc_tr, y_tr)\n",
    "\n",
    "SVM_tuned.set_params(**SVM_tuned_result[0])\n",
    "\n",
    "score_SVM_tuned_tr = SVM_tuned.score(X_pc_tr, y_tr)\n",
    "score_SVM_tuned_val = SVM_tuned_result[1]\n",
    "score_SVM_tuned_ts = SVM_tuned.score(X_pc_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = X_tr.shape[0]\n",
    "\n",
    "n_train = int(n_total*0.8)\n",
    "\n",
    "idx_total = np.arange(n_total)\n",
    "np.random.shuffle(idx_total)\n",
    "\n",
    "idx_train = idx_total[:n_train]\n",
    "idx_val = idx_total[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 변환\n",
    "\n",
    "X_tr_mlp = torch.tensor(X_pc_tr[idx_train])\n",
    "X_val_mlp = torch.tensor(X_pc_tr[idx_val])\n",
    "X_ts_mlp = torch.tensor(X_pc_ts)\n",
    "\n",
    "X_tr_cnn = torch.tensor(X_tr[idx_train]/255)\n",
    "X_val_cnn = torch.tensor(X_tr[idx_val]/255)\n",
    "X_ts_cnn = torch.tensor(X_ts)\n",
    "\n",
    "y_tr_dl = torch.tensor(y_tr[idx_train])\n",
    "y_val_dl = torch.tensor(y_tr[idx_val])\n",
    "y_ts_dl = torch.tensor(y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 만들기 (TensorDataset)\n",
    "\n",
    "train_data_mlp = TensorDataset(X_tr_mlp, y_tr_dl)\n",
    "val_data_mlp = TensorDataset(X_val_mlp, y_val_dl)\n",
    "test_data_mlp = TensorDataset(X_ts_mlp, y_ts_dl)\n",
    "\n",
    "train_data_cnn = TensorDataset(torch.permute(X_tr_cnn,(0,3,1,2)), y_tr_dl)\n",
    "val_data_cnn = TensorDataset(torch.permute(X_val_cnn,(0,3,1,2)), y_val_dl)\n",
    "test_data_cnn = TensorDataset(torch.permute(X_ts_cnn,(0,3,1,2)), y_ts_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader 만들기\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader_mlp = DataLoader(train_data_mlp, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "val_loader_mlp = DataLoader(val_data_mlp, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "test_loader_mlp = DataLoader(test_data_mlp, batch_size = batch_size, shuffle = False, drop_last = False)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_data_cnn, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "val_loader_cnn = DataLoader(val_data_cnn, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "test_loader_cnn = DataLoader(test_data_cnn, batch_size = batch_size, shuffle = False, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 만들기\n",
    "\n",
    "class mlp_classifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim):\n",
    "        super(mlp_classifier, self).__init__()\n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, out_dim)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.layers(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = mlp_classifier(in_dim = X_tr_mlp.shape[1], out_dim = 1032, hid_dim = 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "opt = torch.optim.Adam(mlp_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train code\n",
    "\n",
    "TRAIN_ACCURACY = []\n",
    "TRAIN_LOSS = []\n",
    "VAL_ACCURACY = []\n",
    "VAL_LOSS = []\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    #====================================================#\n",
    "    for data in train_loader_mlp:\n",
    "        x = data[0].type(torch.FloatTensor).to(device)\n",
    "        y_true = data[1].type(torch.LongTensor).to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = mlp_model(x)\n",
    "        y_pred_class = torch.argmax(y_pred, 1) # 행단위 최댓값\n",
    "        loss = loss_fn(y_pred, y_true.flatten())\n",
    "        acc = (y_pred_class == y_true).float().mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_acc += acc\n",
    "        train_loss += loss\n",
    "\n",
    "    train_acc = train_acc/len(train_loader_mlp)\n",
    "    train_loss = train_loss/len(train_loader_mlp)\n",
    "    TRAIN_ACCURACY.append(train_acc)\n",
    "    TRAIN_LOSS.append(train_loss)\n",
    "    #====================================================#\n",
    "\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    for data in val_loader_mlp:\n",
    "        x = data[0].type(torch.FloatTensor).to(device)\n",
    "        y_true = data[1].type(torch.LongTensor).to(device)\n",
    "        y_pred = mlp_model(x)\n",
    "        y_pred_class = torch.argmax(y_pred, 1)\n",
    "        loss = loss_fn(y_pred, y_true.flatten())\n",
    "        acc = (y_pred_class == y_true).float().mean()\n",
    "        \n",
    "\n",
    "        val_acc += acc\n",
    "        val_loss += loss\n",
    "\n",
    "    val_acc = val_acc/len(val_loader_mlp)\n",
    "    val_loss = val_loss/len(val_loader_mlp)\n",
    "    VAL_ACCURACY.append(val_acc)\n",
    "    VAL_LOSS.append(val_loss)\n",
    "    #====================================================#\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print('[EPOCH {}] Tr Acc : {}, Tr Loss : {}, Val Acc : {}, Val Loss : {}'.format(epoch+1,100*train_acc, train_loss, 100*val_acc, val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_classifier(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, num_class = 1032):\n",
    "        super(cnn_classifier, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(*[\n",
    "            nn.Conv2d(in_ch, mid_ch, kernel_size = 3, stride=1, padding = 'same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(mid_ch, mid_ch, kernel_size = 3, stride=1, padding = 'same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(mid_ch, mid_ch, kernel_size = 3, stride=1, padding = 'same'),\n",
    "        ])\n",
    "        self.fc_layer = nn.Linear(2048, num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0),-1) # flatten ( reshape )\n",
    "        y_pred = self.fc_layer(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = cnn_classifier(in_ch = 3, mid_ch = 32, num_class = 1032).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "opt = torch.optim.Adam(cnn_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ACCURACY = []\n",
    "TRAIN_LOSS = []\n",
    "VAL_ACCURACY = []\n",
    "VAL_LOSS = []\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    #====================================================#\n",
    "    for data in train_loader_cnn:\n",
    "        x = data[0].type(torch.FloatTensor).to(device)\n",
    "        y_true = data[1].type(torch.LongTensor).to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = cnn_model(x)\n",
    "        y_pred_class = torch.argmax(y_pred, 1)\n",
    "        loss = loss_fn(y_pred, y_true.flatten())\n",
    "        acc = (y_pred_class == y_true).float().mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_acc += acc\n",
    "        train_loss += loss\n",
    "\n",
    "    train_acc = train_acc/len(train_loader_cnn)\n",
    "    train_loss = train_loss/len(train_loader_cnn)\n",
    "    TRAIN_ACCURACY.append(train_acc)\n",
    "    TRAIN_LOSS.append(train_loss)\n",
    "    #====================================================#\n",
    "\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    for data in val_loader_cnn:\n",
    "        x = data[0].type(torch.FloatTensor).to(device)\n",
    "        y_true = data[1].type(torch.LongTensor).to(device)\n",
    "        y_pred = cnn_model(x)\n",
    "        y_pred_class = torch.argmax(y_pred, 1)\n",
    "        loss = loss_fn(y_pred, y_true.flatten())\n",
    "        acc = (y_pred_class == y_true).float().mean()\n",
    "        \n",
    "\n",
    "        val_acc += acc\n",
    "        val_loss += loss\n",
    "\n",
    "    val_acc = val_acc/len(val_loader_cnn)\n",
    "    val_loss = val_loss/len(val_loader_cnn)\n",
    "    VAL_ACCURACY.append(val_acc)\n",
    "    VAL_LOSS.append(val_loss)\n",
    "    #====================================================#\n",
    "\n",
    "    if (epoch+1)%3 == 0:\n",
    "        print('[EPOCH {}] Tr Acc : {}, Tr Loss : {}, Val Acc : {}, Val Loss : {}'.format(epoch+1,train_acc, train_loss, val_acc, val_loss))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
